{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "class perceptron:\n",
    "    def __init__(self, weights:list, bias=0, output:bool=False) -> None:\n",
    "        self.bias = bias\n",
    "        self.weights = weights\n",
    "        self.output = output\n",
    "        self.activity = 0\n",
    "        self.activation = 0\n",
    "        self.d_weights = weights.copy() # shouldn't use before it gets set\n",
    "        self.d_bias = 0\n",
    "        self.delta = 0\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.calculate_activity(x)\n",
    "        return self.calculate_activation()\n",
    "\n",
    "    def calculate_activity(self, inputs:list):\n",
    "        # A_j = sum(w_ij * x_i) + bias\n",
    "        total = self.bias\n",
    "        for x in range(len(inputs)):\n",
    "            total += inputs[x] * self.weights[x]\n",
    "        self.activity = total\n",
    "        return total\n",
    "    \n",
    "    def calculate_activation(self):\n",
    "        # y_j = f(A_j) -- using sigmoid\n",
    "        self.activation = 1 / (1 + math.exp(self.activity * -1))\n",
    "        return self.activation\n",
    "\n",
    "    def calculate_delta_w(self, x, eta, expected=None, dk_w=None):\n",
    "        # delta_w = eta * (d_k * x_j)\n",
    "        # d_k = e_k (1-y)y\n",
    "        if(self.output):\n",
    "            self.delta = (expected - self.activation) * (1-self.activation) * self.activation\n",
    "        else:\n",
    "            self.delta = (1 - self.activation) * self.activation * (dk_w)\n",
    "        for i in range(len(x)):\n",
    "            self.d_weights[i] = eta * self.delta * x[i]\n",
    "        self.d_bias = eta * self.delta\n",
    "        return self.delta, self.d_weights\n",
    "\n",
    "    def calculate_previous_d(self):\n",
    "        prevs = []\n",
    "        for weight in self.weights:\n",
    "            prevs.append(weight*self.delta)\n",
    "        return prevs\n",
    "\n",
    "    def update_weights(self, bias=True):\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i] = self.weights[i] + self.d_weights[i]\n",
    "        if bias: self.bias = self.bias + self.d_bias\n",
    "        return self.weights\n",
    "    \n",
    "    def compute_error(self, expected):\n",
    "        e = ((expected - self.activation) ** 2) / 2\n",
    "        return e\n",
    "    \n",
    "    def get_weights(self):\n",
    "        return self.weights, self.bias\n",
    "\n",
    "class layer:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: n3 activation: 0.757223870792493, error: 0.0016372856942379682\n",
      "1: n3 activation: 0.7547415782405359, error: 0.0014983201941323589\n"
     ]
    }
   ],
   "source": [
    "iterations = 2\n",
    "x = [1,2]\n",
    "d = .7\n",
    "eta = 1\n",
    "node1 = perceptron([.3,.3], bias=0)\n",
    "node2 = perceptron([.3,.3], bias=0)\n",
    "node3 = perceptron([.8,.8], bias=0, output=True)\n",
    "\n",
    "for i in range(iterations):\n",
    "    hidden_layer_out = []\n",
    "    hidden_layer_out.append(node1.forward(x))\n",
    "    hidden_layer_out.append(node2.forward(x))\n",
    "    output = node3.forward(hidden_layer_out)\n",
    "    print(f\"{i}: n3 activation: {output}, error: {node3.compute_error(d)}\")\n",
    "\n",
    "    node3.calculate_delta_w(hidden_layer_out, eta, expected=d)\n",
    "    prev_deltas = node3.calculate_previous_d()\n",
    "    node1.calculate_delta_w(x, eta, dk_w=prev_deltas[0])\n",
    "    node2.calculate_delta_w(x, eta, dk_w=prev_deltas[1])\n",
    "\n",
    "    node1.update_weights(bias=False)\n",
    "    node2.update_weights(bias=False)\n",
    "    node3.update_weights(bias=False)\n",
    "\n",
    "# Note that the output below is what we get from the module 5.2 slides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "post-1: n3 activation: 0.6583208713508027, error: 0.029204400612317622\n",
      "post-2: n3 activation: 0.3817659623378531, error: 0.055034326882980884\n",
      "Node 3 weights: ([0.9518579656080158, 0.9518579656080158], -0.8257813486188029)\n"
     ]
    }
   ],
   "source": [
    "iterations = 15\n",
    "x1 = [1,1]\n",
    "x2 = [-1,-1]\n",
    "d1 = .9\n",
    "d2 = .05\n",
    "eta = 1\n",
    "node1 = perceptron([.3,.3], bias=0)\n",
    "node2 = perceptron([.3,.3], bias=0)\n",
    "node3 = perceptron([.8,.8], bias=0, output=True)\n",
    "\n",
    "for i in range(iterations):\n",
    "    hidden_layer_out = []\n",
    "    hidden_layer_out.append(node1.forward(x1))\n",
    "    hidden_layer_out.append(node2.forward(x1))\n",
    "    output = node3.forward(hidden_layer_out)\n",
    "    # print(f\"{i}-1: n3 activation: {output}, error: {node3.compute_error(d1)}\")\n",
    "    node3.calculate_delta_w(hidden_layer_out, eta, expected=d1)\n",
    "    prev_deltas = node3.calculate_previous_d()\n",
    "    node1.calculate_delta_w(x1, eta, dk_w=prev_deltas[0])\n",
    "    node2.calculate_delta_w(x1, eta, dk_w=prev_deltas[1])\n",
    "    node1.update_weights()\n",
    "    node2.update_weights()\n",
    "    node3.update_weights()\n",
    "\n",
    "    hidden_layer_out = []\n",
    "    hidden_layer_out.append(node1.forward(x2))\n",
    "    hidden_layer_out.append(node2.forward(x2))\n",
    "    output = node3.forward(hidden_layer_out)\n",
    "    # print(f\"{i}-2: n3 activation: {output}, error: {node3.compute_error(d2)}\")\n",
    "    node3.calculate_delta_w(hidden_layer_out, eta, expected=d2)\n",
    "    prev_deltas = node3.calculate_previous_d()\n",
    "    node1.calculate_delta_w(x2, eta, dk_w=prev_deltas[0])\n",
    "    node2.calculate_delta_w(x2, eta, dk_w=prev_deltas[1])\n",
    "    node1.update_weights()\n",
    "    node2.update_weights()\n",
    "    node3.update_weights()\n",
    "\n",
    "\n",
    "hidden_layer_out = []\n",
    "hidden_layer_out.append(node1.forward(x1))\n",
    "hidden_layer_out.append(node2.forward(x1))\n",
    "output = node3.forward(hidden_layer_out)\n",
    "print(f\"post-1: n3 activation: {output}, error: {node3.compute_error(d1)}\")\n",
    "hidden_layer_out = []\n",
    "hidden_layer_out.append(node1.forward(x2))\n",
    "hidden_layer_out.append(node2.forward(x2))\n",
    "output = node3.forward(hidden_layer_out)\n",
    "print(f\"post-2: n3 activation: {output}, error: {node3.compute_error(d2)}\")\n",
    "print(f\"Node 3 weights: {node3.get_weights()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "post-1: n3 activation: 0.4216576338099176, error: 0.11440570964616345\n",
      "post-2: n3 activation: 0.2838448109486975, error: 0.027341697803816043\n",
      "Node 3 weights: ([0.5894171412879604, 0.5894171412879604], -1.1735567184480915)\n"
     ]
    }
   ],
   "source": [
    "iterations = 15\n",
    "x1 = [1,1]\n",
    "x2 = [-1,-1]\n",
    "d1 = .9\n",
    "d2 = .05\n",
    "eta = 1\n",
    "node1 = perceptron([.3,.3], bias=0)\n",
    "node2 = perceptron([.3,.3], bias=0)\n",
    "node3 = perceptron([.8,.8], bias=0, output=True)\n",
    "\n",
    "for i in range(iterations):\n",
    "    hidden_layer_out = []\n",
    "    hidden_layer_out.append(node1.forward(x1))\n",
    "    hidden_layer_out.append(node2.forward(x1))\n",
    "    output = node3.forward(hidden_layer_out)\n",
    "    # print(f\"{i}-1: n3 activation: {output}, error: {node3.compute_error(d1)}\")\n",
    "    node3.calculate_delta_w(hidden_layer_out, eta, expected=d1)\n",
    "    prev_deltas = node3.calculate_previous_d()\n",
    "    node1.calculate_delta_w(x1, eta, dk_w=prev_deltas[0])\n",
    "    node2.calculate_delta_w(x1, eta, dk_w=prev_deltas[1])\n",
    "    node1.update_weights()\n",
    "    node2.update_weights()\n",
    "    node3.update_weights()\n",
    "\n",
    "for i in range(iterations):\n",
    "    hidden_layer_out = []\n",
    "    hidden_layer_out.append(node1.forward(x2))\n",
    "    hidden_layer_out.append(node2.forward(x2))\n",
    "    output = node3.forward(hidden_layer_out)\n",
    "    # print(f\"{i}-2: n3 activation: {output}, error: {node3.compute_error(d2)}\")\n",
    "    node3.calculate_delta_w(hidden_layer_out, eta, expected=d2)\n",
    "    prev_deltas = node3.calculate_previous_d()\n",
    "    node1.calculate_delta_w(x2, eta, dk_w=prev_deltas[0])\n",
    "    node2.calculate_delta_w(x2, eta, dk_w=prev_deltas[1])\n",
    "    node1.update_weights()\n",
    "    node2.update_weights()\n",
    "    node3.update_weights()\n",
    "\n",
    "\n",
    "hidden_layer_out = []\n",
    "hidden_layer_out.append(node1.forward(x1))\n",
    "hidden_layer_out.append(node2.forward(x1))\n",
    "output = node3.forward(hidden_layer_out)\n",
    "print(f\"post-1: n3 activation: {output}, error: {node3.compute_error(d1)}\")\n",
    "hidden_layer_out = []\n",
    "hidden_layer_out.append(node1.forward(x2))\n",
    "hidden_layer_out.append(node2.forward(x2))\n",
    "output = node3.forward(hidden_layer_out)\n",
    "print(f\"post-2: n3 activation: {output}, error: {node3.compute_error(d2)}\")\n",
    "print(f\"Node 3 weights: {node3.get_weights()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
